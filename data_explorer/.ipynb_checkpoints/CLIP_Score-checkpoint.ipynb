{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9de52f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/zmykevin/miniconda/envs/mmf/lib/python3.7/site-packages/clip/clip.py:23: UserWarning: PyTorch version 1.7.1 or higher is recommended\n",
      "  warnings.warn(\"PyTorch version 1.7.1 or higher is recommended\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from os import path\n",
    "from tqdm import tqdm\n",
    "import html\n",
    "\n",
    "#Image Processing Library\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "\n",
    "import clip\n",
    "import gzip\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab24051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Image Processing\n",
    "preprocess = Compose([\n",
    "    Resize(224, interpolation=Image.BICUBIC),\n",
    "    CenterCrop(224),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
    "# image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc75604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Split Set\n",
    "with open(\"/data/home/zmykevin/vinvl_data/CC/cc_clip_test.json\", \"r\") as f:\n",
    "    clip_test = json.load(f)\n",
    "    \n",
    "#Load the Retrieved Set\n",
    "with open(\"/data/home/zmykevin/vinvl_data/CC/captions_retrieved.json\", \"r\") as f:\n",
    "    cc_captions_retrieved = json.load(f)\n",
    "    \n",
    "#load the caption object data\n",
    "with open(\"/data/home/zmykevin/vinvl_data/CC/cc_objects_captions.json\", \"r\") as f:\n",
    "    cc_objects_captions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e56caa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "bus bus bus bus building bus bus building shirt man shirt person road shirt person person person person man car man man person man bus person bus door person person person person pant person bus person window window person street person window man bus shirt man roof man man shirt person person boat hat balcony van pant bus sign roof\n"
     ]
    }
   ],
   "source": [
    "for k,v in cc_objects_captions.items():\n",
    "    print(k)\n",
    "    print(v[\"objects\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bebab6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'objects': 'drum drum hair microphone dress man drum person woman face shoe shirt person leg hand drum wall shoe woman skirt girl shirt leg poster dress woman leg person arm person picture person tripod neck top', 'objects_no_rep': 'man neck wall woman shirt skirt face top shoe arm person poster hand girl dress tripod drum hair leg picture microphone', 'caption': 'sierra looked stunning in this top and this skirt while performing with person at their former university', 'cc_id': 3239086386}\n",
      "musical microphone on the tripod\n",
      "<class 'torch.Tensor'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#Load the Images\n",
    "CC_Train_Image_Path = \"/fsx/lyuchen2/vilt_dataset/large_experiments/cmd/cc/training\"\n",
    "CC_Val_Image_Path = \"/fsx/lyuchen2/vilt_dataset/large_experiments/cmd/cc/validation\"\n",
    "\n",
    "images = []\n",
    "texts = []\n",
    "\n",
    "for k,v in clip_test.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    \n",
    "    current_image_path = os.path.join(CC_Train_Image_Path, str(v['cc_id'])) if path.exists(os.path.join(CC_Train_Image_Path, str(v['cc_id']))) else os.path.join(CC_Val_Image_Path, str(v['cc_id']))\n",
    "    #print(current_image_path)\n",
    "    assert path.exists(current_image_path)\n",
    "    #current_image_caption = v['caption']\n",
    "    retrieved_image_caption = cc_objects_captions[cc_captions_retrieved[k][0]]['caption']\n",
    "    print(retrieved_image_caption)\n",
    "    current_image = preprocess(Image.open(current_image_path).convert(\"RGB\"))\n",
    "    print(type(current_image))\n",
    "    print(type(current_image_path))\n",
    "    images.append(current_image)\n",
    "    texts.append(current_image_caption)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617f7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mmf)",
   "language": "python",
   "name": "mmf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
