{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6bc836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e89d746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the vinvl mapping\n",
    "def vinvl2cc_mapping(vinvl_annotation, vinvl2cc, vinvl_mapping, missed_mapping):\n",
    "    for k,v in tqdm(vinvl_annotation.items()):\n",
    "        current_caption = v['caption']\n",
    "        vinvl_id = k\n",
    "\n",
    "        if vinvl_mapping.get(current_caption, None) is not None:\n",
    "            cc_id = vinvl_mapping.get(current_caption, None)\n",
    "            vinvl2cc[vinvl_id] = cc_id\n",
    "        else:\n",
    "            missed_mapping.append(vinvl_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ba129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pair the CLIP Image IDs\n",
    "with open(\"/data/home/zmykevin/vinvl_data/CC/cc_objects_captions.json\", \"r\") as f:\n",
    "    cc_objects_captions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f65474f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the original train and validation npy\n",
    "data_path = \"/fsx/lyuchen2/vilt_dataset/large_experiments/cmd/cc\"\n",
    "train_image_path = \"/fsx/lyuchen2/vilt_dataset/large_experiments/cmd/cc/training\"\n",
    "val_image_path = \"/fsx/lyuchen2/vilt_dataset/large_experiments/cmd/cc/validation\"\n",
    "\n",
    "train_annotation = np.load(os.path.join(data_path, \"train_all.npy\"), allow_pickle=True)\n",
    "val_annotation = np.load(os.path.join(data_path, \"val.npy\"), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d973f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapping between id and captions\n",
    "# vinvl_mapping = {}\n",
    "# for k, v in cc_objects_captions.items():\n",
    "#     vinvl_mapping[v['caption']] = k\n",
    "cc_mapping = {}\n",
    "cc_reverse_mapping = {}\n",
    "for ann in train_annotation:\n",
    "    image_id = ann['image_id']\n",
    "    current_caption = ann['captions'][0]\n",
    "    cc_mapping[current_caption] = image_id\n",
    "    cc_reverse_mapping[image_id] = current_caption\n",
    "\n",
    "for ann in val_annotation:\n",
    "    image_id = ann['image_id']\n",
    "    current_caption = ann['captions'][0]\n",
    "    cc_mapping[current_caption] = image_id\n",
    "    cc_reverse_mapping[image_id] = current_caption\n",
    "# print(val_annotation[0])\n",
    "# assert path.exists(os.path.join(val_image_path, str(val_annotation[0]['image_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12b071bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3116254/3116254 [00:08<00:00, 368906.53it/s]\n"
     ]
    }
   ],
   "source": [
    "#Check if we can findthe mapping\n",
    "vinvl2cc = {}\n",
    "missed_mapping = []\n",
    "\n",
    "vinvl2cc_mapping(cc_objects_captions, vinvl2cc, cc_mapping, missed_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4e60760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3055348\n"
     ]
    }
   ],
   "source": [
    "print(len(vinvl2cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0238553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_objects_captions_updated = {}\n",
    "for k,v in cc_objects_captions.items():\n",
    "    vinvl_id = k\n",
    "    vinvl_value = v\n",
    "    #if vinvl2cc.get(vinvl_id, None) is not None:\n",
    "    cc_id = vinvl2cc.get(vinvl_id, None)\n",
    "    vinvl_value['cc_id'] = cc_id\n",
    "    cc_objects_captions_updated[vinvl_id] = vinvl_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72982733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'objects': 'bus bus bus bus building bus bus building shirt man shirt person road shirt person person person person man car man man person man bus person bus door person person person person pant person bus person window window person street person window man bus shirt man roof man man shirt person person boat hat balcony van pant bus sign roof', 'objects_no_rep': 'pant street building shirt man person hat car roof sign door van window road balcony boat bus', 'caption': 'a very typical bus station', 'cc_id': 2901536091}\n"
     ]
    }
   ],
   "source": [
    "for k,v in cc_objects_captions_updated.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f600004",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/home/zmykevin/vinvl_data/CC/cc_objects_captions.json\", \"w\") as f:\n",
    "    json.dump(cc_objects_captions_updated, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dfe2f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60906\n",
      "2163343\n"
     ]
    }
   ],
   "source": [
    "#Find the undetected vinvl data\n",
    "missed_vinvl_data = {}\n",
    "paired_cc_data = {}\n",
    "for k, v in cc_objects_captions.items():\n",
    "    vinvl_id = k\n",
    "    if v['cc_id'] is None:\n",
    "        missed_vinvl_data[k] = v\n",
    "    else:\n",
    "        paired_cc_data[v['cc_id']] = paired_cc_data.get(v['cc_id'], []) + [vinvl_id]\n",
    "print(len(missed_vinvl_data))\n",
    "#Multiple vinvl_id is mapped to one cc_id\n",
    "print(len(paired_cc_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "320d848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick the Test CLIP Set, 10000\n",
    "import random\n",
    "\n",
    "clip_test = {}\n",
    "for k,v in paired_cc_data.items():\n",
    "    if len(v) == 1:\n",
    "        if random.uniform(0,1) > 0.5:\n",
    "            clip_test[v[0]] = cc_objects_captions[v[0]]\n",
    "    if len(clip_test) == 10000:\n",
    "        break\n",
    "\n",
    "with open(\"/data/home/zmykevin/vinvl_data/CC/cc_clip_test.json\", \"w\") as f:\n",
    "    json.dump(clip_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f56c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_bpe():\n",
    "    return os.path.join(\"/checkpoints/zmykevin/models\", \"bpe_simple_vocab_16e6.txt.gz\")\n",
    "\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "class SimpleTokenizer(object):\n",
    "    def __init__(self, bpe_path: str = default_bpe()):\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
    "        merges = merges[1:49152-256-2+1]\n",
    "        merges = [tuple(merge.split()) for merge in merges]\n",
    "        vocab = list(bytes_to_unicode().values())\n",
    "        vocab = vocab + [v+'</w>' for v in vocab]\n",
    "        for merge in merges:\n",
    "            vocab.append(''.join(merge))\n",
    "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
    "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
    "        self.pat = re.compile(r\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\", re.IGNORECASE)\n",
    "        #self.pat = re.compile(r\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]\", re.IGNORECASE)\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token+'</w>'\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        text = whitespace_clean(basic_clean(text)).lower()\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16f93cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer()\n",
    "#text_tokens = [tokenizer.encode(desc) for desc in texts]\n",
    "# text_tokens = []\n",
    "# for desc in texts:\n",
    "#     print(desc)\n",
    "#     text_tokens.append(tokenizer.encoder(desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18dd278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mmf)",
   "language": "python",
   "name": "mmf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
