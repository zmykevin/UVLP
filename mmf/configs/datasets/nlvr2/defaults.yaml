model_config:
  visual_bert:
    hidden_size: 768
    hidden_dropout_prob: 0.1
    training_head_type: nlvr2
    num_labels: 2
    losses:
    - type: cross_entropy

dataset_config:
  nlvr2:
      data_dir: ${env.data_dir}
      depth_first: false
      fast_read: false
      use_images: false
      use_features: true
      features:
          train:
          - /private/home/vedanuj/projects/pythia-internal/data/nlvr2/detectron_fix_100/fc6
          val:
          - /private/home/vedanuj/projects/pythia-internal/data/nlvr2/detectron_fix_100/fc6
          test:
          - /private/home/vedanuj/projects/pythia-internal/data/nlvr2/detectron_fix_100/fc6
      annotations:
          train:
          - imdb/nlvr2/train.jsonl
          val:
          - imdb/nlvr2/dev.jsonl
          test:
          - imdb/nlvr2/test.jsonl
      max_features: 100
      processors:
        text_processor:
          type: vocab
          params:
            max_length: 14
            vocab:
              type: intersected
              embedding_name: glove.6B.300d
              vocab_file: vocabs/vocabulary_100k.txt
            preprocessor:
              type: simple_sentence
              params: {}
        bbox_processor:
          type: bbox
          params:
            max_length: 50
      return_features_info: false
      # Return OCR information
      use_ocr: false
      # Return spatial information of OCR tokens if present
      use_ocr_info: false

optimizer:
  type: adam_w
  params:
    lr: 5e-5
    eps: 1e-8

scheduler:
  type: warmup_linear
  params:
    num_warmup_steps: 6000
    num_training_steps: 60000

evaluation:
  metrics:
  - accuracy

training:
  batch_size: 480
  lr_scheduler: true
  # Don't forget to update schedule_attributes if you update this
  max_updates: 60000
  early_stop:
    criteria: nlvr2/accuracy
    minimize: false

checkpoint:
  pretrained_state_mapping:
    model.bert: model.bert
