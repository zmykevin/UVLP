model_config:
  kd:
    # teacher model config
    teacher:
      type: mmf_transformer
      pretrained_checkpoint: null
      freeze: true
      params:
        transformer_base: xlm-roberta-base
        backend:
          # Follow fairseq.yaml or pytext.yaml under ../mmf_transformer to
          # complete the whole parameter config. The parameters in backends
          # fairseq and pytext are not compatiable and will conflict if a
          # detailed default config is specified here.
          type: fb_pytext
          freeze: false
          params: {}
        heads:
        - type: mlp
          freeze: false
          lr_multiplier: 1.0
          hidden_size: 1024
          num_labels: 5172
        modalities:
        - type: text
          key: text
          position_dim: 512
          embedding_dim: 1024
          segment_id: 0
          layer_norm_eps: 1.0e-12
          hidden_dropout_prob: 0.1
        - type: image
          key: image
          embedding_dim: 2048
          position_dim: 1
          segment_id: 1
          layer_norm_eps: 1.0e-12
          hidden_dropout_prob: 0.1
          encoder:
            type: identity
            params:
              num_output_features: 1
              in_dim: 2048
        initializer_range: 0.02
        initializer_mean: 0.0
        token_noise_std: 0.01
        token_noise_mean: 0.0
        layer_norm_weight_fill: 1.0
        random_initialize: false
        freeze_image_encoder: false
        num_labels: 5172
        hidden_size: 1024
        num_hidden_layers: 24
        num_attention_heads: 16
        vocab_size: 250002
        layer_norm_eps: 1.0e-05
        hidden_dropout_prob: 0.1
        hidden_act: gelu
        max_position_embeddings: 514
        pad_token_id: 1

    # student model config
    student:
      type: mmf_transformer
      pretrained_checkpoint: null
      params:
        transformer_base: xlm-roberta-base
        backend:
          # Follow fairseq.yaml or pytext.yaml under ../mmf_transformer to
          # complete the whole parameter config. The parameters in backends
          # fairseq and pytext are not compatiable and will conflict if a
          # detailed default config is specified here.
          type: fb_pytext
          freeze: false
          params: {}
        heads:
        - type: mlp
          freeze: false
          lr_multiplier: 1.0
          hidden_size: 512
          num_labels: 5172
        modalities:
        - type: text
          key: text
          position_dim: 512
          segment_id: 0
          layer_norm_eps: 1.0e-12
          hidden_dropout_prob: 0.1
        - type: image
          key: image
          embedding_dim: 256
          position_dim: 1
          segment_id: 1
          layer_norm_eps: 1.0e-12
          hidden_dropout_prob: 0.1
          encoder:
            type: identity
            params:
              num_output_features: 1
              in_dim: 256
        initializer_range: 0.02
        initializer_mean: 0.0
        token_noise_std: 0.01
        token_noise_mean: 0.0
        layer_norm_weight_fill: 1.0
        random_initialize: false
        freeze_image_encoder: false
        tie_weight_to_encoder: null
        num_labels: 5172
        hidden_size: 512
        num_hidden_layers: 6
        num_attention_heads: 8
        vocab_size: 250002
        layer_norm_eps: 1.0e-05
        hidden_dropout_prob: 0.1
        hidden_act: gelu
        max_position_embeddings: 514
        pad_token_id: 1
    losses:
    - type: kd_loss
      params:
        distillation_loss:
          loss: softmax_kldiv
          weight: 1
          params: {}
        metric_loss:
          loss: label_smoothing_cross_entropy
          weight: 0.1
          params:
            label_smoothing: 0.1
