includes:
- configs/models/mmf_transformer/defaults.yaml

model_config:
  mmf_transformer:
    heads:
      - type: mlp
        freeze: false
        lr_multiplier: 1.0
        hidden_size: ${model_config.mmf_transformer.hidden_size}
        num_labels: ${model_config.mmf_transformer.num_labels}
    hidden_size: 768
    num_hidden_layers: 12
    num_attention_heads: 12
    vocab_size: 150000
    layer_norm_eps: 1e-5
    hidden_dropout_prob: 0.1
    hidden_act: "gelu"
    max_position_embeddings: 512
    pad_token_id: 1
    backend:
      type: fb_fairseq
      freeze: false
      model_path: manifold://nlp_technologies/tree/xlm/models/prod/cc_fb_30/12_layers/xlm_transformer.pt
      params:
        max_seq_len: 254
        num_segments: 1
        ffn_embedding_dim: 3072
        encoder_normalize_before: True
        export: True
        traceable: True
