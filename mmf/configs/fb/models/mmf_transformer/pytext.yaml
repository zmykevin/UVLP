includes:
- configs/models/mmf_transformer/defaults.yaml

model_config:
  mmf_transformer:
    heads:
      - type: mlp
        freeze: false
        lr_multiplier: 1.0
        hidden_size: ${model_config.mmf_transformer.hidden_size}
        num_labels: ${model_config.mmf_transformer.num_labels}
    hidden_size: 768
    num_hidden_layers: 12
    num_attention_heads: 12
    vocab_size: 250002
    layer_norm_eps: 1e-5
    hidden_dropout_prob: 0.1
    hidden_act: "gelu"
    max_position_embeddings: 514
    pad_token_id: 1
    backend:
      type: fb_pytext
      freeze: false
      params:
        model_path: manifold://nlp_technologies/tree/xlm/models/xlm_r/xlm_transformer.pt-ep12
        is_finetuned: true
        max_seq_len: 514
